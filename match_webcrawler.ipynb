{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Empires webcrawler for matches\n",
    "\n",
    "### Iterative connection to the aoe2.net API\n",
    "\n",
    "A custom crawler is required because the above API restricts downloads to 1000; however this is not enough data to infer meaningful statistics, particularly as the leaderboard of interest (ranked 1v1, leaderboard_id = 3) cannot be pre-filtered through the GET request.\n",
    "\n",
    "Investigation found that the first match after 3 May 2021 00:00 was match_id = 88735565. This crawler works by posting successive requests to the API, iterating the match_id by 1 (n.b. many match_ids are no longer valid, and so lead to bad requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc as db\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API string\n",
    "api_str = 'https://aoe2.net/api/matches?game=aoe2de&count=1000&since='\n",
    "match_start_date = 1620014400  # start point for matches (03 May 2021)\n",
    "end_match_date = 1624766400 # most recent match (27 June 2021)\n",
    "\n",
    "# set up json-to-save structure\n",
    "json_total = list()\n",
    "\n",
    "\n",
    "count = 0\n",
    "current_match_date = match_start_date # this will allow continuing from last URL accessed\n",
    "\n",
    "max_allowed_invalid = 10 # max allowed consecutive connection issues\n",
    "\n",
    "# set up save name structure\n",
    "save_filename_root = 'C:/Users/richa/Documents/AOE2 API Storage/matches{0}.json'\n",
    "save_filename = save_filename_root.format(0)\n",
    "i = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate through jsons from the API\n",
    "invalid_count = 0 # current consecutive url connection errors\n",
    "\n",
    "\n",
    "while current_match_date < end_match_date:\n",
    "    \n",
    "    # every 250,000 matches, switch to a new save filename\n",
    "    if count % 250 == 0 and count != 0:\n",
    "        \n",
    "        # save\n",
    "        with open(save_filename,'w') as outfile:\n",
    "            json.dump(json_total, outfile)\n",
    "            outfile.close()\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        save_filename = save_filename_root.format(i) #change filename\n",
    "        print ('saving to new file:{0}'.format(save_filename))\n",
    "        \n",
    "        json_total = list() # reset match container\n",
    "        \n",
    "    # get the url to access the API\n",
    "    new_match_string = api_str + str(current_match_date)\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(new_match_string,timeout=30) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            \n",
    "    except:\n",
    "        invalid_count += 1\n",
    "        \n",
    "        \n",
    "        if invalid_count > max_allowed_invalid:\n",
    "            print('Too many consecutive connection failures ({0}). Exiting process'.format(max_allowed_invalid))\n",
    "            break\n",
    "        \n",
    "        time.sleep(2)\n",
    "        continue\n",
    "    \n",
    "    # if we get this far, then we have a successful connection. Reset invalid counter\n",
    "    invalid_count = 0\n",
    "    \n",
    "    [json_total.append(i) for i in data]\n",
    "    \n",
    "    current_match_date = int(data[-1]['finished'])\n",
    "    \n",
    "    count += 1\n",
    "    if count % 10 == 0: # save at every 10,000 matches in case of network interruptions\n",
    "        print('Count: {0}. Current matches analysed: {1}. Getting matches after {2}'.format(count,len(json_total),datetime.datetime.fromtimestamp(current_match_date).strftime('%c')))\n",
    "    \n",
    "        \n",
    "        \n",
    "        with open(save_filename,'w') as outfile:\n",
    "            json.dump(json_total, outfile)\n",
    "            outfile.close()\n",
    "            \n",
    "            \n",
    "         \n",
    "# ensure the final records are saved into the latest filename\n",
    "with open(save_filename,'w') as outfile:\n",
    "            json.dump(json_total, outfile)\n",
    "            outfile.close()    \n",
    "print('Completed - all records retrieved. Exiting process.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
